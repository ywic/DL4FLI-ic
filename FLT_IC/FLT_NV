# Relevant libraries and functions
from __future__ import print_function
import mat73

import random
import tensorflow as tf
import keras
from keras.regularizers import l2
#import matplotlib.pyplot as plt
import scipy.ndimage
import numpy as np, h5py
import os, time, sys
import tensorflow as tf
import keras
from keras.regularizers import l2
from keras.models import Model
from keras.layers import BatchNormalization, Convolution2D, Input, SpatialDropout2D, UpSampling2D, MaxPooling2D, concatenate
from keras.layers.core import Activation
from keras.layers import Dense, Dropout, Conv1D, Input, Conv2D, add, Conv3D, Reshape, Layer
from keras.callbacks import History, EarlyStopping, ModelCheckpoint, CSVLogger
from itertools import cycle
from sklearn import metrics
from tensorflow.keras.optimizers import RMSprop
from keras.utils import np_utils
from tensorflow.python.keras.backend import get_session
from tensorflow.keras.layers import PReLU
from tensorflow.keras.layers import Dropout
from keras.layers.convolutional import Convolution2D, MaxPooling2D, SeparableConv2D, Conv2DTranspose
from tensorflow.keras import backend as K
from keras.layers import Lambda,Multiply


import tensorflow as tf

# List all available physical GPUs
physical_devices = tf.config.list_physical_devices('GPU')
print("Available GPUs:", len(physical_devices))

if len(physical_devices) > 0:
    # Select the first GPU and initialize it
    gpu = physical_devices[0]
    tf.config.experimental.set_memory_growth(gpu, True)
    tf.config.experimental.set_visible_devices(gpu, 'GPU')
    
    # Check the cuDNN version
    print("cuDNN version:", tf.test.is_built_with_cuda())
else:
    print("No GPU found.")


#f_data = 'D:\\flt4' # Directory with trainging data
f_data = 'D:\\FLT_\\NV1_noise_small_a3'
stacks = os.listdir(f_data)
numS = int(len(stacks))

nTG = 160 # Number of time-points
xX = 28
yY = 28

tpsfD = np.ndarray(
        (numS, int(nTG), int(xX), int(yY), int(1)), dtype=np.float32
        )
t1 = np.ndarray(
        (numS, int(xX), int(yY), int(1)), dtype=np.float32
        )
t2 = np.ndarray(
        (numS, int(xX), int(yY), int(1)), dtype=np.float32
        )
t3 = np.ndarray(
        (numS, int(xX), int(yY), int(1)), dtype=np.float32
        )
a1 = np.ndarray(
        (numS, int(xX), int(yY), int(1)), dtype=np.float32
        )
a2 = np.ndarray(
        (numS, int(xX), int(yY), int(1)), dtype=np.float32
        )

i = 0;
for d in stacks:
    # Save values to respective mapping
    f = h5py.File(os.path.join(f_data,d),'r') 
    tpsfD[i,:,:,:,0] = f.get('sigD')
    f = h5py.File(os.path.join(f_data,d),'r') 
    t1[i,:,:,0] = f.get('t1')
    f = h5py.File(os.path.join(f_data,d),'r') 
    t2[i,:,:,0] = f.get('t2')
    f = h5py.File(os.path.join(f_data,d),'r') 
    t3[i,:,:,0] = f.get('t3')
    f = h5py.File(os.path.join(f_data,d),'r') 
    a1[i,:,:,0] = f.get('a1')
    f = h5py.File(os.path.join(f_data,d),'r') 
    a2[i,:,:,0] = f.get('a2')
   
    i = i + 1
    
tpsfD =  np.moveaxis(tpsfD, 1, -2)

# Relevant resblock functions (Keras API)
def resblock_2D(num_filters, size_filter, x):
    Fx = Conv2D(num_filters, size_filter, padding='same', activation=None)(x)
    Fx = Activation('relu')(Fx)
    Fx = Conv2D(num_filters, size_filter, padding='same', activation=None)(Fx)
    output = add([Fx, x])
    output = Activation('relu')(output)
    return output

def resblock_2D_BN(num_filters, size_filter, x):
    Fx = Conv2D(num_filters, size_filter, padding='same', activation=None)(x)
    Fx = BatchNormalization()(Fx)
    Fx = Activation('relu')(Fx)
    Fx = Conv2D(num_filters, size_filter, padding='same', activation=None)(Fx)
    Fx = BatchNormalization()(Fx)
    output = add([Fx, x])
    #output = BatchNormalization()(output)
    output = Activation('relu')(output)
    return output

def resblock_3D_BN(num_filters, size_filter, x):
    Fx = Conv3D(num_filters, size_filter, padding='same', activation=None)(x)
    Fx = BatchNormalization()(Fx)
    Fx = Activation('relu')(Fx)
    Fx = Conv3D(num_filters, size_filter, padding='same', activation=None)(Fx)
    Fx = BatchNormalization()(Fx)
    output = add([Fx, x])
    #output = BatchNormalization()(output)
    output = Activation('relu')(output)
    return output

def xCeptionblock_2D_BN(num_filters, size_filter, x):
    Fx = SeparableConv2D(num_filters, size_filter, padding='same', activation=None)(x)
    Fx = BatchNormalization()(Fx)
    Fx = Activation('relu')(Fx)
    Fx = SeparableConv2D(num_filters, size_filter, padding='same', activation=None)(Fx)
    Fx = BatchNormalization()(Fx)
    output = add([Fx, x])
    output = Activation('relu')(output)
    return output

modelD = None
xX = 28;
yY = 28;

t_data = Input(shape=(xX, yY, 160,1))
tpsf = t_data

# # # # # # # # 3D-Model # # # # # # # #

tpsf = Conv3D(50,kernel_size=(1,1,5),strides=(1,1,5), padding='same', activation=None, data_format="channels_last")(tpsf)
tpsf = BatchNormalization()(tpsf)
tpsf = Activation('relu')(tpsf)

#attention = Conv3D(1, kernel_size=(1, 1, 1), activation='sigmoid', padding='same', data_format='channels_last')(tpsf)
# Crop the attention to consider only the first 40 slices
#attention = Lambda(lambda x: x[:, :, :, :80, :])(attention)

# Multiply the original tensor by the attention
#tpsf = Multiply()([tpsf, attention])
tpsf = resblock_3D_BN(50, (1,1,5), tpsf)
print(tpsf.shape[-1]*tpsf.shape[-2])
tpsf = Reshape((xX,yY,1600))(tpsf)
tpsf = Conv2D(256, 1, padding='same', activation=None, data_format="channels_last")(tpsf)
tpsf = BatchNormalization()(tpsf)
tpsf = Activation('relu')(tpsf)
tpsf = Conv2D(256, 1, padding='same', activation=None, data_format="channels_last")(tpsf)
tpsf = BatchNormalization()(tpsf)
tpsf = Activation('relu')(tpsf)
#tpsf = Conv2D(256, 1, padding='same', activation=None, data_format="channels_last")(tpsf)
#tpsf = BatchNormalization()(tpsf)
tpsf = Activation('relu')(tpsf)
tpsf = resblock_2D_BN(256, 1, tpsf)
tpsf = resblock_2D_BN(256, 1, tpsf)

# Short-lifetime branch
imgT1 = Conv2D(64, 1, padding='same', activation=None)(tpsf)
imgT1 = BatchNormalization()(imgT1)
imgT1 = Activation('relu')(imgT1)
imgT1 = Conv2D(32, 1, padding='same', activation=None)(imgT1)
imgT1 = BatchNormalization()(imgT1)
imgT1 = Activation('relu')(imgT1)
imgT1 = Conv2D(1, 1, padding='same', activation=None)(imgT1)
imgT1 = Activation('relu')(imgT1)

# Long-lifetime branch
imgT2 = Conv2D(64, 1, padding='same', activation=None)(tpsf)
imgT2 = BatchNormalization()(imgT2)
imgT2 = Activation('relu')(imgT2)
imgT2 = Conv2D(32, 1, padding='same', activation=None)(imgT2)
imgT2 = BatchNormalization()(imgT2)
imgT2 = Activation('relu')(imgT2)
imgT2 = Conv2D(1, 1, padding='same', activation=None)(imgT2)
imgT2 = Activation('relu')(imgT2)
# Long-lifetime branch
imgT3 = Conv2D(64, 1, padding='same', activation=None)(tpsf)
imgT3 = BatchNormalization()(imgT3)
imgT3 = Activation('relu')(imgT3)
imgT3 = Conv2D(32, 1, padding='same', activation=None)(imgT3)
imgT3 = BatchNormalization()(imgT3)
imgT3 = Activation('relu')(imgT3)
imgT3 = Conv2D(1, 1, padding='same', activation=None)(imgT3)
imgT3 = Activation('relu')(imgT3)
# Amplitude-Ratio branch
imga1 = Conv2D(64, 1, padding='same', activation=None)(tpsf)
imga1 = BatchNormalization()(imga1)
imga1 = Activation('relu')(imga1)
imga1 = Conv2D(32, 1, padding='same', activation=None)(imga1)
imga1 = BatchNormalization()(imga1)
imga1 = Activation('relu')(imga1)
imga1 = Conv2D(1, 1, padding='same', activation=None)(imga1)
imga1 = Activation('relu')(imga1)
# Amplitude-Ratio branch
imga2 = Conv2D(64, 1, padding='same', activation=None)(tpsf)
imga2 = BatchNormalization()(imga2)
imga2 = Activation('relu')(imga2)
imga2 = Conv2D(32, 1, padding='same', activation=None)(imga2)
imga2 = BatchNormalization()(imga2)
imga2 = Activation('relu')(imga2)
imga2 = Conv2D(1, 1, padding='same', activation=None)(imga2)
imga2 = Activation('relu')(imga2)

modelD = Model(inputs=[t_data], outputs=[imgT1,imgT2, imgT3,imga1,imga2])
rmsprop = RMSprop(lr=1e-6)

modelD.compile(loss='mse',
              optimizer=rmsprop,
              metrics=['mae'])

# Setting patience (patience = 15 recommended)
earlyStopping = EarlyStopping(monitor='val_loss', 
                              patience = 20,#15, 
                              verbose = 0,
                              mode = 'auto')

fN = '10knoise' # Assign some name for weights and training/validation loss curves here

# Save loss curve (mse) and MAE information over all trained epochs. (monitor = '' can be changed to focus on other tau parameters)
modelCheckPoint = ModelCheckpoint(filepath=fN+'.h5', 
                                  monitor='val_loss', 
                                  save_best_only=True, 
                                  verbose=0)
subset_size =3000
num_iterations = int(np.ceil(numS / subset_size))

for iteration in range(num_iterations):
    #iteration=iteration +2
    print("Iter",iteration)
    # Define the indices for the subset of data to use in this iteration
    start_index = iteration * subset_size
    end_index = min(start_index + subset_size, numS)
    
    # Get the subset of data for this iteration
    tpsfD_subset = tpsfD[start_index:end_index]
    t1_subset = t1[start_index:end_index]
    t2_subset = t2[start_index:end_index]
    t3_subset = t3[start_index:end_index]
    a1_subset = a1[start_index:end_index]
    a2_subset = a2[start_index:end_index]
        # Train the model on this subset of data
    history = History()
    csv_logger = CSVLogger(fN + '_' + str(iteration) + '.log')
    modelD.fit([tpsfD_subset], [t1_subset, t2_subset,t3_subset, a1_subset,a2_subset],
               validation_split=0.2,
               batch_size=4, epochs=500, verbose=1, shuffle=True,
               callbacks=[earlyStopping, csv_logger, modelCheckPoint])
    
    # Save the trained model after each iteration if desired
    modelD.save(fN + '_' + str(iteration) + '.h5')














import numpy as np
import scipy.io as sio
# Load the data from the .mat file
#data = sio.loadmat('D:\\flt4exp\A_09_592.mat')
data = mat73.loadmat('D:\\FLT_exp\\val\\b_00002_1.mat')
#data = mat73.loadmat('D:\\FLT_exp\\val\\bi_sim_val.mat')

#data = mat73.loadmat('D:\\FLT_\\NV4_uni\\b_00002_1.mat')
# Get the data array from the dictionary
A = data['sigD']

A_reshaped = A.reshape(1, 28, 28, 160, 1)

# Print the shape of the array
print(A_reshaped.shape)  # Output: (1, 160, 28, 28, 1)
modelD.load_weights("FLTmultiGPUbi.h5")

# Perform inference on test data with trained model
testV = modelD.predict(A_reshaped)
t1P = testV[0] # Predicted t1 values
t2P = testV[1] # Predicted t2 values
t3P = testV[2] # Predicted t2 values
a1P = testV[3] # Predicted AR values
a2P = testV[4] # Predicted AR values
